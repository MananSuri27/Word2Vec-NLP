# Word2Vec-NLP

Medium Article: [A Dummy's Guide to Word2Vec](https://medium.com/@manansuri/a-dummys-guide-to-word2vec-456444f3c673)

Word2Vec creates a representation of each word present in our vocabulary into a vector. Words used in similar contexts or having semantic relationships are captured effectively through their closeness in the vector space- effectively speaking similar words will have similar word vectors! History. Word2vec was created, patented, and published in 2013 by a team of researchers led by Tomas Mikolov at Google.

Hypothetical features to understand word embeddings![Hypothetical features to understand word embeddings](https://miro.medium.com/max/1050/1*Z-EOcLtUDlhvVuaruyM8EQ.png)

We can easily train word2vec word embeddings using Gensim, which is, *“is a free open-source Python library for representing documents as semantic vectors, as efficiently (computer-wise) and painlessly (human-wise) as possible.”*



